{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(d2l.Decoder):  #@save\n",
    "    \"\"\"The base attention-based decoder interface.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttentionDecoder(AttentionDecoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0):\n",
    "        super().__init__()\n",
    "        self.attention = d2l.AdditiveAttention(num_hiddens, dropout)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = nn.LazyLinear(vocab_size)\n",
    "        self.apply(d2l.init_seq2seq)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens):\n",
    "        outputs, hidden_state = enc_outputs\n",
    "        # print(outputs)\n",
    "        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)\n",
    "    \n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, hidden_state, enc_valid_lens = state\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        outputs, self._attention_weights = [], []\n",
    "        for x in X:\n",
    "            query = torch.unsqueeze(hidden_state[-1], dim=1)\n",
    "            context = self.attention(query, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "            \n",
    "            print(context.shape)\n",
    "            print(context)\n",
    "            print(\"----------------------------------------------------------------\")\n",
    "            print(torch.unsqueeze(x, dim=1).shape)\n",
    "            print(torch.unsqueeze(x, dim=1))\n",
    "\n",
    "            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=1)\n",
    "\n",
    "            out, hidden_state = self.rnn(x.permute(1, 0 ,2))\n",
    "            outputs.append(out)\n",
    "            self._attention_weights.append(self.attention.attention_weights)\n",
    "        outputs = self.dense(torch.cat(outputs, dim=0))\n",
    "        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state, enc_valid_lens]\n",
    "    \n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 16])\n",
      "tensor([[[ 0.0963, -0.2351,  0.1959, -0.1839, -0.1110, -0.1178, -0.3405,\n",
      "           0.2456,  0.3421, -0.3238,  0.2807, -0.2502,  0.4350, -0.4681,\n",
      "          -0.1753, -0.3188]],\n",
      "\n",
      "        [[ 0.0963, -0.2351,  0.1959, -0.1839, -0.1110, -0.1178, -0.3405,\n",
      "           0.2456,  0.3421, -0.3238,  0.2807, -0.2502,  0.4350, -0.4681,\n",
      "          -0.1753, -0.3188]],\n",
      "\n",
      "        [[ 0.0963, -0.2351,  0.1959, -0.1839, -0.1110, -0.1178, -0.3405,\n",
      "           0.2456,  0.3421, -0.3238,  0.2807, -0.2502,  0.4350, -0.4681,\n",
      "          -0.1753, -0.3188]],\n",
      "\n",
      "        [[ 0.0963, -0.2351,  0.1959, -0.1839, -0.1110, -0.1178, -0.3405,\n",
      "           0.2456,  0.3421, -0.3238,  0.2807, -0.2502,  0.4350, -0.4681,\n",
      "          -0.1753, -0.3188]]], grad_fn=<BmmBackward0>)\n",
      "----------------------------------------------------------------\n",
      "torch.Size([4, 1, 8])\n",
      "tensor([[[-0.4258, -1.1284, -0.9827,  1.3844, -1.3728,  1.7386, -0.5952,\n",
      "           0.7795]],\n",
      "\n",
      "        [[-0.4258, -1.1284, -0.9827,  1.3844, -1.3728,  1.7386, -0.5952,\n",
      "           0.7795]],\n",
      "\n",
      "        [[-0.4258, -1.1284, -0.9827,  1.3844, -1.3728,  1.7386, -0.5952,\n",
      "           0.7795]],\n",
      "\n",
      "        [[-0.4258, -1.1284, -0.9827,  1.3844, -1.3728,  1.7386, -0.5952,\n",
      "           0.7795]]], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 16 but got size 8 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((batch_size, num_steps), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m      7\u001b[0m state \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39minit_state(encoder(X), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m output, state \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m d2l\u001b[38;5;241m.\u001b[39mcheck_shape(output, (batch_size, num_steps, vocab_size))\n\u001b[0;32m     10\u001b[0m d2l\u001b[38;5;241m.\u001b[39mcheck_shape(state[\u001b[38;5;241m0\u001b[39m], (batch_size, num_steps, num_hiddens))\n",
      "File \u001b[1;32md:\\Codez\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Codez\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[35], line 29\u001b[0m, in \u001b[0;36mSeq2SeqAttentionDecoder.forward\u001b[1;34m(self, X, state)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39munsqueeze(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39munsqueeze(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m out, hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m ,\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m     32\u001b[0m outputs\u001b[38;5;241m.\u001b[39mappend(out)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 16 but got size 8 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\n",
    "batch_size, num_steps = 4, 7\n",
    "encoder = d2l.Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "decoder = Seq2SeqAttentionDecoder(vocab_size, embed_size, num_hiddens,\n",
    "                                  num_layers)\n",
    "X = torch.zeros((batch_size, num_steps), dtype=torch.long)\n",
    "state = decoder.init_state(encoder(X), None)\n",
    "output, state = decoder(X, state)\n",
    "d2l.check_shape(output, (batch_size, num_steps, vocab_size))\n",
    "d2l.check_shape(state[0], (batch_size, num_steps, num_hiddens))\n",
    "d2l.check_shape(state[1][0], (batch_size, num_hiddens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
